<!DOCTYPE html>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



<style>
    
#dmimage {
transition: transform 0.5s;
}
#dmimage:hover {
transform: scale(1.2);
}

#figimage {
transition: transform 0.5s;
}
#figimage:hover {
transform: scale(1.1);
}


body { padding : 20px 0; font-size : 20px; background-image : url("bg.png");} 

.img-circle {
    border-radius: 50%;
}

.shadow {box-shadow: rgba(0, 0, 0, 0.15) 6.00px 6.00px 2.6px;}
.shadow2  {box-shadow: rgba(0, 0, 0, 0.16) 0px 3px 6px, rgba(0, 0, 0, 0.23) 0px 3px 6px;}
.leftSideBar{display:none !important;}
.rightSideBar{display:none !important;}

.site-footer {

  text-align: center;
  margin: 0px 0;
  padding: 0px 0;
}

#social-wrapper {
  text-align: center;
}

/*Social Media Icons*/
.social-wrapper {
  text-align: center;
}

.social-wrapper ul li {
  display: inline;
  margin: 0 5px;
}

.twitter-icon,
.facebook-icon,
.instagram-icon,
.linkedin-icon,
.googleplus-icon,
.youtube-icon,
.foursquare-icon{
  margin-top: .625em;
  width: 20px;
  height: 20px;
  opacity: .6;
  filter: alpha(opacity=60); /* For IE8 and earlier */
  border-radius: 25px;
}

.twitter-icon:hover,
.facebook-icon:hover,
.instagram-icon:hover,
.linkedin-icon:hover,
.googleplus-icon:hover,
.youtube-icon:hover,
.foursquare-icon:hover {
  opacity: 1.0;
  filter: alpha(opacity=100); /* For IE8 and earlier */
}

.footer-nav p {
  text-align: center;
}

.column {
  float: left;
  padding: 5px;
}

.left {
  width: 35%;

}

.right {
  width: 63%;

}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}

/*@media screen and (max-width: 600px) {
  .column {
    width: 100%;
  }
}*/

.tabs {
  margin: 20px;
  padding: 0;
  list-style: none;
  position: relative;
  border-bottom: 1px solid #ccc;
}
.tabs .active-tab {
  border-top: 1px solid #ccc;
  border-left: 1px solid #ccc;
  border-right: 1px solid #ccc;
  border-bottom: none;
  position: relative;
  color: black;
}
.tabs .active-tab:after {
  width: 100%;
  height: 2px;
  position: absolute;
  content: "";
  bottom: -0.1em;
  left: 0;
  background: white;
}
.tabs li {
  display: inline-block;
  cursor: pointer;
  color: #3a5ea7;
  padding: 5px 10px;
}
.tabs li:first-child {
  margin-left: 10px;
}

.tabs-content {
  margin: 20px;
  padding: 0;
  list-style: none;
}
.tabs-content li {
  display: none;
}

* {box-sizing: border-box}
body {font-family: Verdana, sans-serif; margin:0}
.mySlides {display: none}
img {vertical-align: middle;}

/* Slideshow container */
.slideshow-container {
  max-width: 1000px;
  position: relative;
  margin: auto;
}

/* Next & previous buttons */
.prev, .next {
  cursor: pointer;
  position: absolute;
  top: 50%;
  width: auto;
  padding: 10px;
  margin-left: -50%;
  color: black;
  font-weight: bold;
  font-size: 18px;
  transition: 0.6s ease;
  border-radius: 0 3px 3px 0;
  user-select: none;
}

/* Position the "next button" to the right */
.next {
  right: 0;
  border-radius: 3px 0 0 3px;
}

/* On hover, add a black background color with a little bit see-through */
.prev:hover, .next:hover {
  background-color: rgba(0,0,0,0.8);
}

/* Caption text */
.text {
  color: #f2f2f2;
  font-size: 15px;
  padding: 8px 12px;
  position: absolute;
  bottom: 8px;
  width: 100%;
  text-align: center;
}

/* Number text (1/3 etc) */
.numbertext {
  color: #f2f2f2;
  font-size: 12px;
  padding: 8px 12px;
  position: absolute;
  top: 0;
}

/* The dots/bullets/indicators */
.dot {
  cursor: pointer;
  height: 15px;
  width: 15px;
  margin: 0 2px;
  background-color: #bbb;
  border-radius: 50%;
  display: inline-block;
  transition: background-color 0.6s ease;
}

.active, .dot:hover {
  background-color: #717171;
}

/* Fading animation */
.fade {
  animation-name: fade;
  animation-duration: 1.5s;
}

@keyframes fade {
  from {opacity: .4} 
  to {opacity: 1}
}

/* On smaller screens, decrease text size */
@media only screen and (max-width: 300px) {
  .prev, .next,.text {font-size: 11px}
}

@media only screen and (max-width: 600px) {
  .github {display: none}
}


</style>



<!-- Please Remove the below lines if you are using my code-->
<!-- Google Analytics -->
<script src="./Ankan Kumar Bhunia - Homepage_files/track.php" async=""></script><script async="" src="./Ankan Kumar Bhunia - Homepage_files/analytics.js.download"></script><script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-143450446-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<script type="text/javascript">(function(){var hstc=document.createElement('script'); hstc.src='https://log.hitsteps.com/track.php?code=422b10447cdcc581710fb4fc8399b0ba';hstc.async=true;var htssc = document.getElementsByTagName('script')[0];htssc.parentNode.insertBefore(hstc, htssc);})();
</script><noscript><a href="http://www.hitsteps.com/"><img src="//log.hitsteps.com/track.php?mode=img&amp;code=422b10447cdcc581710fb4fc8399b0ba" alt="web statistics" width="1" height="1" />web statistics</a></noscript>	
<script async src="//static.getclicky.com/101263529.js"></script>
<script async="" src="./Ankan Kumar Bhunia - Homepage_files/js"></script>
<!-- Please Remove the below lines if you are using my code#-->


  <script async="" src="./src1/js"></script>



  <title>Ankan Kumar Bhunia - Homepage</title>
  
  <meta name="author" content="Jon Barron">
 <!-- <meta name="viewport" content="width=device-width, initial-scale=1" /> -->
<meta name="viewport" content="width=700">

  
  <link rel="stylesheet" type="text/css" href="./src1/stylesheet.css">
 <link rel = "icon" type = "image/png" href = "./src2/ju.png">
</head>



<body  data-new-gr-c-s-check-loaded="14.1023.0" data-gr-ext-installed="">
  <div style="font-family:Courier; ">
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;background-color : #fff;box-shadow: 0px 0px 10px #999;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
			
			 <div id="container">
			  <div class="column left">
			<table border="0" cellpadding="0" cellspacing="4">

			  <tbody style="text-align:center"><tr><td valign="top" rowspan="6"><img id="dmimage" height="100" border="0" class="img-circle" src="im1.png" onmouseover="changeImage()" onmouseout="restoreImage()">
			</tbody>
			  <td style="text-align:center" valign="top" colspan="2"><span class="h1"><h2>Ankan Kumar Bhunia</h2></span>
			  
			    <p style="text-align:center">
				Computer Vision Researcher<br>
                <a href="mailto:ankankumarbhunia@gmail.com">Email</a> &nbsp;/&nbsp;
                <a href="https://ankanbhunia.github.io/Resume/Ankan_Resume.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=2leAc3AAAAAJ&amp;hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://github.com/ankanbhunia">GitHub</a>&nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/ankan-kumar-bhunia/">Linkedin</a>
              </p></td>
        </table>
 
 
     

      
    <h2>Recent News</h2>
   
    <b>[10/2023]</b>: 1 patent at <a href="https://patents.google.com/patent/US11756244B1/en/">US Patent</a><br>
     <b>[07/2023]</b>: 1 paper at <a href="https://iccv2023.thecvf.com/">ICCV 2023</a><br>
    <b>[06/2023]</b>: 1 paper at <a href="https://conferences.miccai.org/2023/en/">MICCAI 2023</a><br>
    <b>[02/2023]</b>: 1 paper at <a href="http://cvpr2023.thecvf.com/">CVPR 2023</a><br>
	<b>[07/2022]</b>: 1 paper at <a href="https://eccv2022.ecva.net/">ECCV 2022</a><br>
	<b>[07/2021]</b>: 1 paper at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a><br>
	<b>[10/2019]</b>: 1 paper at <a href="https://www.journals.elsevier.com/information-fusion">Information Fusion</a><br>
    <b>[07/2019]</b>: 1 paper at <a href="https://www.journals.elsevier.com/pattern-recognition">Pattern Recognition</a><br>
    <b>[03/2019]</b>: 1 paper at <a href="http://cvpr2019.thecvf.com//">CVPR 2019</a><br>
	<b>[05/2019]</b>: 1 paper at <a href="http://2019.ieeeicip.org/">ICIP 2019</a><br>
    <!-- <b>[05/2019]</b>: 1 paper at <a href="https://link.springer.com/journal/521">Neural Computing and Application</a><br> -->
	<b>[08/2018]</b>: 1 paper at <a href="https://www.journals.elsevier.com/pattern-recognition">Pattern Recognition</a><br>
    <b>[04/2018]</b>: 3 papers at <a href="http://www.icpr2018.org/">ICPR 2018</a><br>

	  
<p></p>
    <h2 style="display:inline;">Education</h2>
    <!--<div style="top:-10px; position:relative;">-->

    <p></p>
  
    <table border="0" cellpadding="0" cellspacing="4">

    <tbody><tr><td valign="middle" rowspan="6"><img height="60" border="0", src="./src2/uoe.png">

    </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>University of Edinburgh, UK</b></span>
  
    <br><em>PhD at School of Informatics</em><br> 
          May 2023 - Present
  </td></tr></tbody></table>
     
	<p></p>
	
  	<table border="0" cellpadding="0" cellspacing="4">

	  <tbody><tr><td valign="middle" rowspan="6"><img height="60" border="0", src="./src2/ju.png">

	  </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>Jadavpur University, India</b></span>
	
	  <br><em>B.E. in Electrical Engineering</em><br> 
          July 2016 - May 2020
	</td></tr></tbody></table>

		
		
<p></p>
    <h2 style="display:inline;">Research Experiences</h2>
		<p></p>
	
  	 <table border="0" cellpadding="0" cellspacing="4">

	  <tbody><tr><td valign="middle" rowspan="6"><img height="60" border="0" src="./src2/mbzuai.png">

	  </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>MBZUAI, Abu Dhabi</b></span>
	
	  <br><em>Research Assistant</em>.<br> 
	  Nov 2020 - Apr 2023 </a>
	  
	</td></tr></tbody></table>
	
	<p></p>
	
  	<table border="0" cellpadding="0" cellspacing="4">

	  <tbody><tr><td valign="middle" rowspan="6"><img height="60" border="0" src="./src2/UOM.png">

	  </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>University of Manitoba, Winnipeg, Canada</b></span>
	
	  <br><em>MITACS Internship</em>, 2019.<br> 
	  May 2019 - August 2019 </a>
	  
	</td></tr></tbody></table>
	<p></p>
	<table border="0" cellpadding="0" cellspacing="4">

	  <tbody><tr><td valign="middle" rowspan="6"><img height="60" border="0" src="./src2/bosch.jpg">

	  </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>Robert Bosch, Bangalore</b></span>
	
	  <br><em>Computer Vision Lab</em><br> 
	  June 2018 - July 2018 </tbody></table>
		<p></p>
	<table border="0" cellpadding="0" cellspacing="4">

	  <tbody><tr><td valign="middle" rowspan="6"><img height="60" border="0" src="./src2/iitr.png">

	  </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>IIT Roorkee, India</b></span>
	
	  <br><em>Under Prof. Partha Pratim Roy</em><br>
	  June 2017 - May 2020
	</td></tr></tbody></table>



</div>

<div class="column right" style="margin-left:-10px">


  
 
<!--   	  <h2>Research</h2>
	  <p>
		I am broadly interested in computer vision, machine learning, and AI. I mostly work on generative models and visual perception models from a small amount of data. 
	  </p> -->
    
<br>
      <p style="padding-left: 15px; padding-right: 25px; text-align: justify">
         
   <font style="color: black;"> I'm a PhD student in the <a href="https://www.ed.ac.uk/informatics">School of Informatics</a> at the <a href="https://en.wikipedia.org/wiki/University_of_Edinburgh">University of Edinburgh</a>. I work with <a href=https://homepages.inf.ed.ac.uk/hbilen/>Dr. Hakan Bilen</a> in the <a href=https://groups.inf.ed.ac.uk/vico/>Visual Computing Group</a> (VICO).</font> 
	 <font style="color: black;"> My secondary supervisor is <a href=https://enigma-li.github.io/>Dr. Changjian Li</a> from GraphVix lab. Previously, I was a full-time research assistant at MBZUAI, Abu Dhabi, where I worked on computer vision and machine learning with <a href="https://sites.google.com/view/fahadkhans/home">Dr Fahad Shahbaz Khan</a>, <a href="https://salman-h-khan.github.io/">Dr Salman Khan</a> and <a href="https://scholar.google.com/citations?hl=en&user=bZ3YBRcAAAAJ&view_op=list_works&sortby=pubdate">Dr Hisham Cholakkal</a>. During my undergrad, I have worked with <a href="https://users.encs.concordia.ca/~wayang/">Dr Yang Wang</a> at the University of Manitoba, Canada and <a href="http://parimal.iitr.ac.in/people/partha-pratim-roy">Dr Partha Roy</a> at IIT Roorkee, India. I did my bachalors from <a href="https://en.wikipedia.org/wiki/Jadavpur_University">Jadavpur University</a>, Kolkata in Electrical Engineering.

    </p>

<ul class="tabs">
  <li class="active-tab">Publications</li>
  <li>Research</li>
  <!-- <li>CV/Resume</li> -->
  <li>Contact</li>
</ul>

<ul class="tabs-content">
  <li>
<div style="overflow-y: scroll; height:950px;">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

             <div style="margin-left:15px;"> More details are available on <a href="https://scholar.google.com/citations?user=2leAc3AAAAAJ&hl=en">Google Scholar</a></div>

               <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
              <img id="figimage" class=shadow src="./src2/gmnr.png" height="90", width="120" style="margin-left:10px;">

            </td>              
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.01172">
                <papertitle>Generative Multiplane Neural Radiance for 3D-Aware Image Generation</papertitle>              </a>
                 - [<a href="https://arxiv.org/abs/2304.01172"><em style="color: #767872;">paper</em></a>] / [<a href="https://github.com/VIROBO-15/GMNR"><em style="color: #767872;">code</em></a>] 
              <br>
              Amandeep Kumar,
              <strong>Ankan Kumar Bhunia</strong>, Sanath Narayan, Hisham Cholakkal,
              Rao Anwer,
              <aa href="https://salman-h-khan.github.io/">Salman Khan</a>, Ming-Hsuan Yang,
              <aa href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>,
              <br>
              <strong style="color: indianred;"><em>ICCV</em>, 2023</strong><br>
              <!-- -->


          <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
              <img id="figimage" class=shadow src="./src2/pidm.png" height="90", width="120" style="margin-left:10px;">

            </td>              
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.12500">
                <papertitle>Person Image Synthesis via Denoising Diffusion Model</papertitle>              </a>

             
                - [<a href="https://arxiv.org/abs/2211.12500"><em style="color: #767872;">paper</em></a>] / [<a href="https://github.com/ankanbhunia/PIDM"><em style="color: #767872;">code</em></a>] / [<a href="https://colab.research.google.com/github/ankanbhunia/PIDM/blob/main/PIDM_demo.ipynb"><em style="color: #767872;">demo</em></a>] / [<a href="https://ankanbhunia.github.io/PIDM/"><em style="color: #767872;">webpage</em></a>] <br>
              <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://salman-h-khan.github.io/">Salman Khan</a>,
              <aa href="https://scholar.google.com/citations?user=bZ3YBRcAAAAJ&hl=en">Hisham Cholakkal</a>,
              <aa href="https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en">Rao Anwer</a>,
        <aa href="https://scholar.google.com/citations?user=qQP6WXIAAAAJ&hl=en">Jorma Laaksonen</a>,
        <aa href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en">Mubarak Shah</a>,
              <aa href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>,
       
              <br>
              <strong style="color: indianred;"><em>CVPR</em>, 2023</strong><br>
              <!-- -->

<!-- <iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=PIDM&type=star&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="90" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=PIDM&type=watch&count=true&size=large&v=2&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=PIDM&type=fork&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>       -->        
            </td>
          </tr>

          <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
              <img id="figimage" class=shadow src="./src2/doodles.png" height="90", width="120" style="margin-left:10px;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.03258">
                <papertitle>DoodleFormer: Creative Sketch Drawing with Transformers</papertitle>
              </a>
              - [<a href="https://arxiv.org/abs/2112.03258"><em style="color: #767872;">paper</em></a>] / [<a href="https://github.com/ankanbhunia/doodleformer"><em style="color: #767872;">code</em></a>] / [<a href="https://ankanbhunia.github.io/doodleformer/"><em style="color: #767872;">webpage</em></a>] 
              <br>
              <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://salman-h-khan.github.io/">Salman Khan</a>,
              <aa href="https://scholar.google.com/citations?user=bZ3YBRcAAAAJ&hl=en">Hisham Cholakkal</a>,
              <aa href="https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en">Rao Anwer</a>,
              <aa href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>,
        <aa href="https://scholar.google.com/citations?user=qQP6WXIAAAAJ&hl=en">Jorma Laaksonen</a>,
        <aa href="https://scholar.google.com/citations?user=lkWfR08AAAAJ&hl=en">Michael Felsberg</a>,
              <br>
              <strong style="color: indianred;"><em>ECCV</em>, 2022</strong>

              
            </td>
          </tr>

          <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
              <img id="figimage" class=shadow src="./src2/hwt.png" height="90", width="120" style="margin-left:10px;">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.03964">
                <papertitle>Handwriting Transformers</papertitle>
              </a>
              - [<a href="https://arxiv.org/abs/2104.03964"><em style="color: #767872;">paper</em></a>] / [<a href="https://github.com/ankanbhunia/Handwriting-Transformers"><em style="color: #767872;">code</em></a>] / [<a href="https://colab.research.google.com/github/ankanbhunia/Handwriting-Transformers/blob/main/demo.ipynb"><em style="color: #767872;">demo</em></a>] / [<a href="https://ankanbhunia.github.io/Handwriting-Transformers/"><em style="color: #767872;">webpage</em></a>] 
              <br>
              <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://salman-h-khan.github.io/">Salman Khan</a>,
              <aa href="https://scholar.google.com/citations?user=bZ3YBRcAAAAJ&hl=en">Hisham Cholakkal</a>,
              <aa href="https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en">Rao Anwer</a>,
              <aa href="https://sites.google.com/view/fahadkhans/home">Fahad Khan</a>,
        <aa href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en">Mubarak Shah</a>,
              <br>
              <strong style="color: indianred;"><em>ICCV</em>, 2021</strong>

              
            </td>
          </tr>

      
          <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im1.png' height="90", width="120" style="margin-left:10px;">  
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html">
                <papertitle>Handwriting Recognition in Low-resource Scripts using Adversarial Learning</papertitle>
              </a> - [<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html"><em style="color: #767872;">paper</em></a>]
              <br>
        
              <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
              <aa href="https://scholar.google.com/citations?user=lvPxKQ0AAAAJ&hl=en">Abhirup Das</a>,
        <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://sairajk.github.io/">Sairaj Kishore</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
              <br>
              <strong style="color: indianred;"><em>CVPR</em>, 2019</strong>

            </td>
          </tr>
      
          <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im2.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1810.11120">
                <papertitle>Improving Document Binarization via Adversarial Noise-Texture Augmentation</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1810.11120"><em style="color: #767872;">paper</em></a>]
              <br>
        <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
              <aa href="https://aneeshan95.github.io/">Aneeshan Sain</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
              <br>
              <strong style="color: indianred;"><em>ICIP</em>, 2019</strong>

            </td>
          </tr>     
    
              <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/PR_logo.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1811.01395">
                <papertitle>A Deep One-Shot Network for Query-based Logo Retrieval</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1811.01395"><em style="color: #767872;">paper</em></a>]
              <br>
        
              <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
        <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://shuvozitghose.github.io/">Shuvozit Ghose</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
        <aa href="https://scholar.google.co.in/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
              <br>
              <strong style="color: indianred;"><em>Pattern Recognition</em>, 2019</strong>

            </td>
          </tr>   
    
    
      <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im4.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1801.00470">
                <papertitle>Script identification in natural scene image and video frames using an attention based Convolutional-LSTM network</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1801.00470"><em style="color: #767872;">paper</em></a>]
              <br>
        
              
        <strong>Ankan Kumar Bhunia</strong>,
              <aa href="https://sites.google.com/site/konweraishik/">Aishik Konwer</a>,
        <aa href="">Abir Bhowmik</a>,
        <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
      
              <br>
              <strong style="color: indianred;"><em>Pattern Recognition</em>, 2019</strong>

            </td>
          </tr>   
      
                    <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im5.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1709.09348">
                <papertitle>Signature Verification Approach using Fusion of Hybrid Texture Features</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1709.09348"><em style="color: #767872;">paper</em></a>]
              <br>
        
        <strong>Ankan Kumar Bhunia</strong>,
        <aa href="">Alireza Alaei</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,

              <br>
              <strong style="color: indianred;"><em>Neural computing and Applications</em>, 2019</strong>
            </td>
          </tr>   
        <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im6.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1801.07156">
                <papertitle>Word Level Font-to-Font Image Translation using Convolutional Recurrent Generative Adversarial Networks</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1801.07156"><em style="color: #767872;">paper</em></a>]
              <br>
        <strong>Ankan Kumar Bhunia</strong>,
        <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
        <aa href="">Prithaj Banerjee</a>,
              <aa href="https://sites.google.com/site/konweraishik/">Aishik Konwer</a>,
        <aa href="">Abir Bhowmik</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
        <aa href="https://scholar.google.co.in/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
              <br>
              <strong style="color: indianred;"><em>ICPR</em>, 2018</strong>

            </td>
          </tr>   
      
        <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im7.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1801.07141">
                <papertitle>Staff line Removal using Generative Adversarial Networks</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1801.07141"><em style="color: #767872;">paper</em></a>]
              <br>
        <aa href="https://sites.google.com/site/konweraishik/">Aishik Konwer</a>,
        <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
        <aa href="">Abir Bhowmik</a>,
        <strong>Ankan Kumar Bhunia</strong>,
        <aa href="">Prithaj Banerjee</a>,

              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
        <aa href="https://scholar.google.co.in/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
              <br>
              <strong style="color: indianred;"><em>ICPR</em>, 2018</strong>
            </td>
          </tr>   
      
              <tr>
            <td style="width:200px;height:100px; margin-left:20px;margin-top:15px;">
                <img id="figimage" class=shadow src='./src2/im8.png' height="90", width="120" style="margin-left:10px;"> 
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1801.07211">
                <papertitle>Handwriting Trajectory Recovery using End-to-End Deep Encoder-Decoder Network</papertitle>
              </a> - [<a href="https://arxiv.org/abs/1801.07211"><em style="color: #767872;">paper</em></a>]
              <br>
        
        <aa href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
        <aa href="">Abir Bhowmik</a>,
        <strong>Ankan Kumar Bhunia</strong>,
        <aa href="https://sites.google.com/site/konweraishik/">Aishik Konwer</a>,
        <aa href="">Prithaj Banerjee</a>,
              <aa href="https://scholar.google.com/citations?user=moDpyKkAAAAJ&hl=en">Partha Pratim Roy</a>,
        <aa href="https://scholar.google.co.in/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a>,
              <br>
              <strong style="color: indianred;"><em>ICPR</em>, 2018</strong>
            </td>
          </tr>   
      
            
      
        </tbody></table>

</div>


</li>
<li style='text-align: center;'><br>
<div class="slideshow-container">

<div class="mySlides fade">
  <div style='width:90%; margin: auto;' >

  <strong style="color:black">Title: Person Image Synthesis via Denoising Diffusion Model </strong> <em style="color:green"> (CVPR'23)</em> <br> [<a href="https://arxiv.org/abs/2211.12500">Paper</a> / <a href="https://github.com/ankanbhunia/PIDM">GitHub</a> / <a href="https://ankanbhunia.github.io/PIDM/">Project</a> / <a href="https://colab.research.google.com/github/ankanbhunia/PIDM/blob/main/PIDM_demo.ipynb"  style="color:red"><b>Demo</b></a>]<br><br>
<div class='github'>
  <iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=PIDM&type=star&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="95" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=PIDM&type=watch&count=true&size=large&v=2&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=PIDM&type=fork&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe> </div>
<br><img src="./gifs/pidm.gif" alt="this slowpoke moves"  width="100%"/><br>
  <p  style='text-align:justify; font-size: 13px; color:gray'>
  The Pose-guided person image synthesis task aims to render a person’s image with a desired pose and appearance. Specifically, the appearance is defined by a given source image and the pose by a set of keypoints. Having control over the synthesized person images in terms of pose and style is an important requisite for applications such as e-commerce, virtual reality, metaverse and content generation for the entertainment industry. Read our paper <a href="https://arxiv.org/abs/2211.12500">here</a>.
  </p>

  </div>

</div>

<div class="mySlides fade">
   <div  style='width:90%; margin: auto;'>

  <strong style="color:black">Title: Generative Multiplane Neural Radiance for 3D-Aware Image Generation</strong> <em style="color:green"> (ICCV'23)</em> [<a href="https://arxiv.org/pdf/2304.01172.pdf">Paper</a> / <a href="https://github.com/VIROBO-15/GMNR">GitHub</a>]<br><br>
<div class='github'>
  <iframe src="https://ghbtns.com/github-btn.html?user=VIROBO-15&repo=GMNR&type=star&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="95" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=VIROBO-15&repo=GMNR&type=watch&count=true&size=large&v=2&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=VIROBO-15&repo=GMNR&type=fork&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>    </div>

 <br><img src="./gifs/gmnr.gif" alt="this slowpoke moves"  width="100%"/><br>
    <p  style='text-align:justify; font-size: 13px; color:gray'>
  We introduced an approach, named GMNR, that focuses at efficiently generating 3D-aware high-resolution images that are view-consistent across multiple camera pose. Qualitative and quantitative experiments on three datasets
demonstrate the merits of our contributions, leading to favorable performance in terms of image generation quality
and computational efficiency, compared to existing works.

  </p>

  </div>
</div>

<div class="mySlides fade">
   <div  style='width:90%; margin: auto;'>

  <strong style="color:black">Title: Handwriting Transformers</strong>  <em style="color:green"> (ICCV'21)</em> <br>  [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhunia_Handwriting_Transformers_ICCV_2021_paper.pdf">Paper</a> / <a href="https://github.com/ankanbhunia/Handwriting-Transformers">GitHub</a> / <a href="https://colab.research.google.com/github/ankanbhunia/Handwriting-Transformers/blob/main/demo.ipynb" style="color:red"><b>Demo</b></a>]<br><br>
<div class='github'>

  <iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=Handwriting-Transformers&type=star&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="95" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=Handwriting-Transformers&type=watch&count=true&size=large&v=2&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=Handwriting-Transformers&type=fork&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe> </div>

<br><img src="./gifs/hwt.gif" alt="this slowpoke moves"  height="150px" width="100%"/><br>
    <p  style='text-align:justify; font-size: 13px; color:gray'>
Automatic handwritten text generation can be beneficial for people having disabilities or injuries that prevent them from writing, adapting an author's writing style or gathering additional data for training deep learning-based handwritten text recognition models. Here, we investigate the problem of realistic handwritten text generation of text sequences with arbitrary length and diverse calligraphic attributes representing writing styles of a writer. For more visit our project page <a href="https://ankanbhunia.github.io/Handwriting-Transformers/">here</a>.

  </p>
  
  </div>
</div>

<div class="mySlides fade">
   <div  style='width:90%; margin: auto;'>

  <strong style="color:black">Title: Doodleformer: Creative sketch drawing with transformers</strong>  <em style="color:green"> (ECCV'22)</em> <br>  [<a href="https://arxiv.org/abs/2112.03258">Paper</a> / <a href="https://github.com/ankanbhunia/doodleformer">GitHub</a>]<br><br>
<div class='github'>

  <iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=doodleformer&type=star&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="95" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=doodleformer&type=watch&count=true&size=large&v=2&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>
<iframe src="https://ghbtns.com/github-btn.html?user=ankanbhunia&repo=doodleformer&type=fork&count=true&size=small" frameborder="0" style="margin-top:0px;" scrolling="0" width="100" height="20"  title="GitHub"></iframe>  </div>
  <br><img src="./gifs/doodleformer.jpeg" alt="this slowpoke moves" height="150px" width="100%"/><br>
    <p  style='text-align:justify; font-size: 13px; color:gray'>

Humans have an outstanding ability to easily communicate and express abstract ideas
and emotions through sketch drawings. Creative sketching or doodling is an expressive activity, where imaginative and previously unseen depictions of everyday visual objects are drawn. Creative sketch image generation is a challenging vision problem, where the task is to generate diverse, yet realistic creative sketches possessing the unseen composition of the visual-world objects. Please visit our project page <a href="https://ankanbhunia.github.io/doodleformer/">here</a>.
  </p>
  </div>  <div class="text"></div>
</div>

<!-- <div class="mySlides fade">
  <div class="numbertext">3 / 3</div>
  <img src="./src2/uoe.png" style="width:100%">
  <div class="text">Caption Three</div>
</div> -->


<a class="prev" onclick="plusSlides(-1)">❮</a>
<a class="next" onclick="plusSlides(1)">❯</a>

</div>
<br>

<div style="text-align:center">
  <span class="dot" onclick="currentSlide(1)"></span> 
  <span class="dot" onclick="currentSlide(2)"></span> 
  <span class="dot" onclick="currentSlide(3)"></span> 
  <span class="dot" onclick="currentSlide(4)"></span> 
</div>

  </li>


<!--   <li>
  <iframe src="./Resume/Ankan_Resume.pdf" height="900" width="540"></iframe>

  </li> -->
  <li style='text-align: center; font-family:courier;'>
<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d1556.1956136720185!2d-3.187865156749468!3d55.9450301321123!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x4887c78381d04457%3A0xd072ab38eb50c7a4!2sSchool%20of%20Informatics!5e0!3m2!1sen!2suk!4v1684098165835!5m2!1sen!2suk" width="300" height="225" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe><br><br>

    The Bayes Centre, Edinburgh EH8 9BT <br>
    Email: ankankumarbhunia@gmail.com / <br>
    a.k.bhunia@sms.ed.ac.uk<br>
    Phone: +44-7770723790 <br>
    Whatsapp: +91-9038858890

  </li>
</ul>

      </td>
    </tr>
  </tbody></table>
<hr>
<footer id="colophon" class="site-footer" role="contentinfo" style="margin-left: -30px;">
  <div class="social-wrapper">
    <ul>
      <li>
        <a href="https://twitter.com/ankanbh" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/twitter.png" alt="Twitter Logo" class="twitter-icon"></a>
      </li>
      <li>
        <a href="https://wa.me/919038858890" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/wa.png" alt="Instagram Logo" class="instagram-icon"></a>
      </li>
            <li>
        <a href="https://www.linkedin.com/in/ankan-kumar-bhunia/" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/linkedin.png" alt="Instagram Logo" class="instagram-icon"></a>
      </li>

      <li>
        <a href="https://www.youtube.com/channel/UC2G-oDOz9FTJN-b839_mZtA" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/youtube.png" alt="Youtube Logo" class="youtube-icon"></a>
      </li>
      <li>
        <a href="https://github.com/ankanbhunia" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/github.png" alt="Foursquare Logo" class="foursquare-icon"></a>
      </li>
            <li>
        <a href="http://scholar.google.com/citations?user=2leAc3AAAAAJ&hl=en" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/gsc.png" alt="Foursquare Logo" class="foursquare-icon"></a>
      </li>
            <li>
        <a href="https://dblp.org/pid/206/7200" target="_blank">
          <img src="https://raw.githubusercontent.com/ankanbhunia/ankanbhunia.github.io/master/icons/dblp.png" alt="Foursquare Logo" class="foursquare-icon"></a>
      </li>
    </ul>
  </div>

</footer>
<!-- #colophon -->

</div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>

<script>
let slideIndex = 1;
showSlides(slideIndex);

function plusSlides(n) {
  showSlides(slideIndex += n);
}

function currentSlide(n) {
  showSlides(slideIndex = n);
}

function showSlides(n) {
  let i;
  let slides = document.getElementsByClassName("mySlides");
  let dots = document.getElementsByClassName("dot");
  if (n > slides.length) {slideIndex = 1}    
  if (n < 1) {slideIndex = slides.length}
  for (i = 0; i < slides.length; i++) {
    slides[i].style.display = "none";  
  }
  for (i = 0; i < dots.length; i++) {
    dots[i].className = dots[i].className.replace(" active", "");
  }
  slides[slideIndex-1].style.display = "block";  
  dots[slideIndex-1].className += " active";
}
</script>
<script>
$(function () {

  var activeIndex = $('.active-tab').index(),
      $contentlis = $('.tabs-content li'),
      $tabslis = $('.tabs li');
  
  // Show content of active tab on loads
  $contentlis.eq(activeIndex).show();

  $('.tabs').on('click', 'li', function (e) {
    var $current = $(e.currentTarget),
        index = $current.index();
    
    $tabslis.removeClass('active-tab');
    $current.addClass('active-tab');
    $contentlis.hide().eq(index).show();
   });
});

</script>
<script>
    function changeImage() {
        document.getElementById("dmimage").src = "im2.png";
    }

    function restoreImage() {
        document.getElementById("dmimage").src = "im1.png";
    }
</script>
</body>


